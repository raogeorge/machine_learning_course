{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_normalize_data(filename):\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    \n",
    "    X_normalized = (X - mean) / std\n",
    "    \n",
    "    X_normalized = np.hstack((np.ones((X_normalized.shape[0], 1)), X_normalized))\n",
    "    \n",
    "    return X_normalized, y, mean, std\n",
    "\n",
    "def calculate_cost(X, y, w):\n",
    "    m = X.shape[0]\n",
    "    h = X.dot(w)\n",
    "    return np.sum((h - y)**2) / (2*m)\n",
    "\n",
    "def batch_gradient_descent(X, y, learning_rate, max_iterations=10000, tolerance=1e-5):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        h = X.dot(w)\n",
    "        gradient = X.T.dot(h - y) / m\n",
    "        w_new = w - learning_rate * gradient\n",
    "        \n",
    "        cost = calculate_cost(X, y, w)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        if np.linalg.norm(w_new - w) < tolerance:\n",
    "            print(f\"BGD converged after {i+1} iterations\")\n",
    "            return w_new, costs\n",
    "        \n",
    "        w = w_new\n",
    "    \n",
    "    print(\"BGD did not converge\")\n",
    "    return w, costs\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        idx = np.random.randint(0, m)\n",
    "        xi, yi = X[idx], y[idx]\n",
    "        \n",
    "        prediction = np.dot(xi, w)\n",
    "        gradient = xi * (prediction - yi)\n",
    "        w -= learning_rate * gradient\n",
    "        \n",
    "        if i % 100 == 0:  # Calculate cost every 100 iterations to reduce computation\n",
    "            cost = calculate_cost(X, y, w)\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return w, costs\n",
    "\n",
    "def normal_equation(X, y):\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m load_and_normalize_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m load_and_normalize_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Batch Gradient Descent\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_and_normalize_data('train.csv')\n",
    "X_test, y_test = load_and_normalize_data('test.csv')\n",
    "\n",
    "# Batch Gradient Descent\n",
    "bgd_lr = 0.001  # You may need to adjust this\n",
    "bgd_w, bgd_costs = batch_gradient_descent(X_train, y_train, bgd_lr)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "sgd_lr = 0.001  # You may need to adjust this\n",
    "num_iterations = 10000\n",
    "sgd_w, sgd_costs = stochastic_gradient_descent(X_train, y_train, sgd_lr, num_iterations)\n",
    "\n",
    "# Normal Equation\n",
    "ne_w = normal_equation(X_train, y_train)\n",
    "\n",
    "print(\"BGD weight vector:\", bgd_w)\n",
    "print(\"SGD weight vector:\", sgd_w)\n",
    "print(\"Normal Equation weight vector:\", ne_w)\n",
    "\n",
    "# Calculate costs\n",
    "bgd_test_cost = calculate_cost(X_test, y_test, bgd_w)\n",
    "sgd_test_cost = calculate_cost(X_test, y_test, sgd_w)\n",
    "ne_test_cost = calculate_cost(X_test, y_test, ne_w)\n",
    "\n",
    "print(f\"BGD test cost: {bgd_test_cost}\")\n",
    "print(f\"SGD test cost: {sgd_test_cost}\")\n",
    "print(f\"Normal Equation test cost: {ne_test_cost}\")\n",
    "\n",
    "# Plot cost functions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bgd_costs, label='BGD')\n",
    "plt.plot(range(0, num_iterations, 100), sgd_costs, label='SGD')\n",
    "plt.axhline(y=ne_test_cost, color='r', linestyle='--', label='Normal Equation')\n",
    "plt.title('Cost Function vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
