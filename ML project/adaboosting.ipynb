{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Added this import\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "train_data = pd.read_csv(\"train_final.csv\")\n",
    "test_data = pd.read_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "numeric_features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "categorical_features = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(train_data, test_data):\n",
    "    train_cleaned = train_data.copy()\n",
    "    test_cleaned = test_data.copy()\n",
    "    \n",
    "    for feature in ['workclass', 'occupation', 'native.country']:\n",
    "        mode_value = train_data[feature][train_data[feature] != '?'].mode()[0]\n",
    "        train_cleaned[feature] = train_cleaned[feature].replace('?', mode_value)\n",
    "        test_cleaned[feature] = test_cleaned[feature].replace('?', mode_value)\n",
    "    \n",
    "    return train_cleaned, test_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost Classification Results:\n",
      "================================\n",
      "Base Estimator: Decision Tree\n",
      "Number of estimators: 100\n",
      "Learning rate: 0.1\n",
      "\n",
      "Cross-validation metrics:\n",
      "Accuracy:\n",
      "  Mean: 0.871 (+/- 0.006)\n",
      "Precision:\n",
      "  Mean: 0.799 (+/- 0.014)\n",
      "Recall:\n",
      "  Mean: 0.623 (+/- 0.026)\n",
      "F1:\n",
      "  Mean: 0.700 (+/- 0.017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "                              feature  importance\n",
      "3                        capital.gain    0.136452\n",
      "0                                 age    0.135823\n",
      "2                       education.num    0.099944\n",
      "29  marital.status_Married-civ-spouse    0.097219\n",
      "5                      hours.per.week    0.083633\n",
      "4                        capital.loss    0.063563\n",
      "1                              fnlwgt    0.048129\n",
      "56                           sex_Male    0.031065\n",
      "51                  relationship_Wife    0.024945\n",
      "40           occupation_Other-service    0.023951\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "train_data_cleaned, test_data_cleaned = handle_missing_values(train_data, test_data)\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create base decision tree (weak learner)\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=3,              # Shallow trees as weak learners\n",
    "    min_samples_split=30,\n",
    "    min_samples_leaf=15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create AdaBoost Classifier pipeline\n",
    "adaboost_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', AdaBoostClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=100,     # Number of weak learners\n",
    "        learning_rate=0.1,    # Contribution of each classifier\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Prepare target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data_cleaned['income>50K'])\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(adaboost_model, \n",
    "                          train_data_cleaned.drop('income>50K', axis=1), \n",
    "                          y_train, \n",
    "                          cv=5, \n",
    "                          scoring=scoring)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nAdaBoost Classification Results:\")\n",
    "print(\"================================\")\n",
    "print(\"Base Estimator: Decision Tree\")\n",
    "print(\"Number of estimators: 100\")\n",
    "print(\"Learning rate: 0.1\")\n",
    "print(\"\\nCross-validation metrics:\")\n",
    "for metric in scoring.keys():\n",
    "    scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    print(f\"  Mean: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "\n",
    "# Fit the model on full training data\n",
    "adaboost_model.fit(train_data_cleaned.drop('income>50K', axis=1), y_train)\n",
    "\n",
    "# Generate predictions for test data\n",
    "test_ids = test_data_cleaned['ID'].copy()\n",
    "test_predictions = adaboost_model.predict(test_data_cleaned.drop('ID', axis=1))\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'income>50K': label_encoder.inverse_transform(test_predictions)\n",
    "})\n",
    "\n",
    "# Feature importance analysis\n",
    "def get_feature_importance(pipeline, feature_names):\n",
    "    categorical_features_encoded = pipeline.named_steps['preprocessor']\\\n",
    "        .named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "    all_features = numeric_features + list(categorical_features_encoded)\n",
    "    \n",
    "    importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': all_features,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    return feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_importance = get_feature_importance(adaboost_model, numeric_features + categorical_features)\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Save predictions\n",
    "submission.to_csv('adaboost_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost Classification Results:\n",
      "================================\n",
      "Base Estimator: Decision Tree\n",
      "Number of estimators: 100\n",
      "Learning rate: 0.01\n",
      "\n",
      "Cross-validation metrics:\n",
      "Accuracy:\n",
      "  Mean: 0.847 (+/- 0.010)\n",
      "Precision:\n",
      "  Mean: 0.775 (+/- 0.022)\n",
      "Recall:\n",
      "  Mean: 0.516 (+/- 0.037)\n",
      "F1:\n",
      "  Mean: 0.619 (+/- 0.031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\raoge\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "                              feature  importance\n",
      "29  marital.status_Married-civ-spouse    0.446050\n",
      "2                       education.num    0.218059\n",
      "3                        capital.gain    0.139500\n",
      "0                                 age    0.137232\n",
      "5                      hours.per.week    0.031257\n",
      "4                        capital.loss    0.027871\n",
      "14                     education_12th    0.000031\n",
      "6                 workclass_Local-gov    0.000000\n",
      "7              workclass_Never-worked    0.000000\n",
      "8                   workclass_Private    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create base decision tree (weak learner)\n",
    "base_estimator = DecisionTreeClassifier(\n",
    "    max_depth=3,              # Shallow trees as weak learners\n",
    "    min_samples_split=30,\n",
    "    min_samples_leaf=15,\n",
    "    random_state=2\n",
    ")\n",
    "\n",
    "# Create AdaBoost Classifier pipeline\n",
    "adaboost_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', AdaBoostClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=100,     # Number of weak learners\n",
    "        learning_rate=0.01,    # Contribution of each classifier\n",
    "        random_state=2\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Prepare target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data_cleaned['income>50K'])\n",
    "\n",
    "# Perform cross-validation with multiple metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(adaboost_model, \n",
    "                          train_data_cleaned.drop('income>50K', axis=1), \n",
    "                          y_train, \n",
    "                          cv=5, \n",
    "                          scoring=scoring)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nAdaBoost Classification Results:\")\n",
    "print(\"================================\")\n",
    "print(\"Base Estimator: Decision Tree\")\n",
    "print(\"Number of estimators: 100\")\n",
    "print(\"Learning rate: 0.01\")\n",
    "print(\"\\nCross-validation metrics:\")\n",
    "for metric in scoring.keys():\n",
    "    scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.capitalize()}:\")\n",
    "    print(f\"  Mean: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "\n",
    "# Fit the model on full training data\n",
    "adaboost_model.fit(train_data_cleaned.drop('income>50K', axis=1), y_train)\n",
    "\n",
    "# Generate predictions for test data\n",
    "test_ids = test_data_cleaned['ID'].copy()\n",
    "test_predictions = adaboost_model.predict(test_data_cleaned.drop('ID', axis=1))\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'income>50K': label_encoder.inverse_transform(test_predictions)\n",
    "})\n",
    "\n",
    "# Feature importance analysis\n",
    "def get_feature_importance(pipeline, feature_names):\n",
    "    categorical_features_encoded = pipeline.named_steps['preprocessor']\\\n",
    "        .named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "    all_features = numeric_features + list(categorical_features_encoded)\n",
    "    \n",
    "    importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': all_features,\n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    return feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_importance = get_feature_importance(adaboost_model, numeric_features + categorical_features)\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "submission.to_csv('adaboost_predictions2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
