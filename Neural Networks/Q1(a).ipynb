{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np  # for numerical computations\n",
    "import pandas as pd  # for data handling\n",
    "\n",
    "# Sigmoid activation function: squashes input to range [0,1]\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function, used in backpropagation\n",
    "# Note: if s = sigmoid(x), then s'(x) = s(x)(1-s(x))\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def initialize_weights(input_size, hidden_sizes, output_size):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for all layers of the network\n",
    "    \n",
    "    Parameters:\n",
    "    - input_size: number of features (4 for bank-note dataset)\n",
    "    - hidden_sizes: list specifying number of neurons in each hidden layer [5,3]\n",
    "    - output_size: number of output neurons (1 for binary classification)\n",
    "    \"\"\"\n",
    "    weights = []  # list to store weight matrices\n",
    "    biases = []   # list to store bias vectors\n",
    "    \n",
    "    # Input layer → First hidden layer\n",
    "    # Shape: (hidden_sizes[0], input_size) = (5,4)\n",
    "    # Each row represents weights for one neuron in first hidden layer\n",
    "    weights.append(np.random.randn(hidden_sizes[0], input_size) * 0.01)\n",
    "    # Shape: (hidden_sizes[0], 1) = (5,1)\n",
    "    biases.append(np.zeros((hidden_sizes[0], 1)))\n",
    "    \n",
    "    # Between hidden layers\n",
    "    for i in range(1, len(hidden_sizes)):\n",
    "        # Shape: (hidden_sizes[i], hidden_sizes[i-1]) = (3,5)\n",
    "        # Connects each layer to next layer\n",
    "        weights.append(np.random.randn(hidden_sizes[i], hidden_sizes[i-1]) * 0.01)\n",
    "        biases.append(np.zeros((hidden_sizes[i], 1)))\n",
    "    \n",
    "    # Last hidden layer → Output layer\n",
    "    # Shape: (output_size, hidden_sizes[-1]) = (1,3)\n",
    "    weights.append(np.random.randn(output_size, hidden_sizes[-1]) * 0.01)\n",
    "    biases.append(np.zeros((output_size, 1)))\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "def forward_propagation(X, weights, biases):\n",
    "    \"\"\"\n",
    "    Compute forward pass through the network\n",
    "    \n",
    "    Parameters:\n",
    "    - X: input features for one example, shape (4,1)\n",
    "    - weights: list of weight matrices\n",
    "    - biases: list of bias vectors\n",
    "    \n",
    "    Returns:\n",
    "    - activations: list of outputs from each layer\n",
    "    - zs: list of weighted inputs to each layer before activation\n",
    "    \"\"\"\n",
    "    activations = [X]  # list starts with input layer\n",
    "    zs = []  # store weighted inputs before activation\n",
    "    \n",
    "    # For each layer's weights and biases\n",
    "    for W, b in zip(weights, biases):\n",
    "        # Compute weighted sum plus bias: z = Wx + b\n",
    "        z = np.dot(W, activations[-1]) + b\n",
    "        # Apply sigmoid activation: a = sigmoid(z)\n",
    "        a = sigmoid(z)\n",
    "        # Store intermediate values for backprop\n",
    "        zs.append(z)\n",
    "        activations.append(a)\n",
    "    \n",
    "    return activations, zs\n",
    "\n",
    "def backward_propagation(X, y, weights, biases, activations, zs):\n",
    "    \"\"\"\n",
    "    Compute gradients using backpropagation\n",
    "    \n",
    "    Parameters:\n",
    "    - X: input features for one example\n",
    "    - y: true label\n",
    "    - weights, biases: network parameters\n",
    "    - activations: list of outputs from forward pass\n",
    "    - zs: list of weighted inputs from forward pass\n",
    "    \n",
    "    Returns:\n",
    "    - gradients_w: list of gradients for weight matrices\n",
    "    - gradients_b: list of gradients for bias vectors\n",
    "    \"\"\"\n",
    "    gradients_w = [np.zeros_like(W) for W in weights]\n",
    "    gradients_b = [np.zeros_like(b) for b in biases]\n",
    "    \n",
    "    # Output layer error (delta)\n",
    "    # For binary cross-entropy loss with sigmoid output\n",
    "    delta = activations[-1] - y.reshape(-1, 1)\n",
    "    \n",
    "    # Gradient for last layer\n",
    "    gradients_w[-1] = np.dot(delta, activations[-2].T)\n",
    "    gradients_b[-1] = delta\n",
    "    \n",
    "    # Backpropagate through hidden layers\n",
    "    # l counts backwards through layers\n",
    "    for l in range(len(weights) - 2, -1, -1):\n",
    "        # delta = (next layer's weights)^T * (next layer's delta) * sigmoid'(z)\n",
    "        delta = np.dot(weights[l + 1].T, delta) * sigmoid_derivative(zs[l])\n",
    "        gradients_w[l] = np.dot(delta, activations[l].T)\n",
    "        gradients_b[l] = delta\n",
    "    \n",
    "    return gradients_w, gradients_b\n",
    "\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the bank-note dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - train_path: path to training data CSV\n",
    "    - test_path: path to test data CSV\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, y_train: training features and labels\n",
    "    - X_test, y_test: test features and labels\n",
    "    \"\"\"\n",
    "    # Read CSV files\n",
    "    train_data = pd.read_csv(train_path, header=None)\n",
    "    test_data = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    # Split into features (X) and labels (y)\n",
    "    # .T transposes to get shape (n_features, n_examples)\n",
    "    X_train = train_data.iloc[:, :-1].values.T  \n",
    "    y_train = train_data.iloc[:, -1].values    \n",
    "    X_test = test_data.iloc[:, :-1].values.T   \n",
    "    y_test = test_data.iloc[:, -1].values      \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Example initialization\n",
    "np.random.seed(2)  \n",
    "input_size = 4     # bank-note has 4 features\n",
    "hidden_sizes = [5, 3]  # 5 neurons in first hidden layer, 3 in second\n",
    "output_size = 1    # binary classification needs 1 output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight gradients: [array([[-1.30830685e-05, -3.45220159e-05,  1.31102675e-05,\n",
      "         1.43570026e-05],\n",
      "       [-2.90244479e-05, -7.65861962e-05,  2.90847882e-05,\n",
      "         3.18506376e-05],\n",
      "       [ 2.14603795e-05,  5.66270489e-05, -2.15049945e-05,\n",
      "        -2.35500352e-05],\n",
      "       [-2.58771272e-05, -6.82814277e-05,  2.59309244e-05,\n",
      "         2.83968537e-05],\n",
      "       [-1.01970952e-05, -2.69068592e-05,  1.02182944e-05,\n",
      "         1.11900142e-05]]), array([[ 0.0008482 ,  0.00080029,  0.0007455 ,  0.00080613,  0.0009144 ],\n",
      "       [ 0.00031307,  0.00029538,  0.00027516,  0.00029754,  0.0003375 ],\n",
      "       [-0.00052667, -0.00049692, -0.0004629 , -0.00050055, -0.00056778]]), array([[0.24931508, 0.2485087 , 0.25075493]])]\n",
      "Bias gradients: [array([[-3.39987748e-06],\n",
      "       [-7.54253993e-06],\n",
      "       [ 5.57687676e-06],\n",
      "       [-6.72465040e-06],\n",
      "       [-2.64990390e-06]]), array([[ 0.00170377],\n",
      "       [ 0.00062885],\n",
      "       [-0.00105792]]), array([[0.50125685]])]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "X_train, y_train, X_test, y_test = load_data(train_path, test_path)\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights, biases = initialize_weights(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Forward propagation\n",
    "activations, zs = forward_propagation(X_train[:, [0]], weights, biases) \n",
    "\n",
    "# Backward propagation\n",
    "gradients_w, gradients_b = backward_propagation(X_train[:, [0]], y_train[[0]], weights, biases, activations, zs)\n",
    "\n",
    "print(\"Weight gradients:\", gradients_w)\n",
    "print(\"Bias gradients:\", gradients_b)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
